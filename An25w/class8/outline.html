Almost sure convergence · Convergence in probability · Convergence in distribution


1. Almost sure convergence

Let $(X_n)_{n\ge 1}$ and $X$ be random variables defined on the same probability space $(\Omega,\mathcal F,\mathbb P)$. We say that $X_n$ converges to $X$ almost surely if
$$
\mathbb P\big(\{\omega \in \Omega : \lim_{n\to\infty} X_n(\omega) = X(\omega)\}\big)=1.
$$

Example (indicator bump). Take the uniform probability measure on $[0,1]$ and set
$$
X_n(\omega)=\begin{cases}
1, & \omega < \tfrac{1}{n},\\
0, & \omega \ge \tfrac{1}{n}.
\end{cases}
$$
Then $X_n(\omega) \to 0$ for every $\omega>0$, so $X_n \to 0$ almost surely.

Stability. If $X_n \to X$ and $Y_n \to Y$ almost surely, then $X_n+Y_n \to X+Y$ and $X_nY_n \to XY$ almost surely. Indeed, on $A_X = \{\omega : X_n(\omega) \to X(\omega)\}$ and $A_Y = \{\omega : Y_n(\omega) \to Y(\omega)\}$ we have convergence of sums and products; both events have probability one, so their intersection does as well.


2. Convergence in probability

Definition. We say $X_n$ converges to $X$ in probability, written $X_n \xrightarrow{\mathbb P} X$, if for every $\varepsilon>0$,
$$
\lim_{n\to\infty} \mathbb P\big(|X_n - X| > \varepsilon\big)=0.
$$

Example. For the indicator sequence above we already know $X_n \to 0$ almost surely, hence also in probability.

Addition rule. If $X_n \xrightarrow{\mathbb P} X$ and $Y_n \xrightarrow{\mathbb P} Y$, then $X_n + Y_n \xrightarrow{\mathbb P} X + Y$. For any $\varepsilon>0$,
$$
\mathbb P\big(|(X_n+Y_n)-(X+Y)|>\varepsilon\big) \le 
\mathbb P\big(|X_n-X|>\tfrac{\varepsilon}{2}\big) +
\mathbb P\big(|Y_n-Y|>\tfrac{\varepsilon}{2}\big) \xrightarrow[n\to\infty]{} 0.
$$

Product rule (to report). With a similar argument based on $|X_nY_n-XY| \le |X_n||Y_n-Y| + |Y||X_n-X|$, one shows $X_nY_n \xrightarrow{\mathbb P} XY$.


3. Convergence in distribution

Definition. $X_n$ converges in distribution to $X$, written $X_n \Rightarrow X$, if for every bounded continuous function $f$ we have
$$
\lim_{n\to\infty} \mathbb E[f(X_n)] = \mathbb E[f(X)].
$$

Example. Since $X_n \to 0$ in probability, it follows that $X_n \Rightarrow 0$.

Non-closure under addition. There exist sequences where $X_n \Rightarrow X$ and $Y_n \Rightarrow Y$, but $X_n + Y_n$ fails to converge in distribution to $X+Y$. Work on $([0,1],\mathcal B,\text{Leb})$ and let $U$ be uniform. Set $X = \mathbf 1_{\{U\le 1/2\}}$ and $Y = X$, so $X+Y$ takes the values $0$ and $2$ with equal probability. Now define $X_n = X$ for all $n$, while $Y_n = X$ when $n$ is odd and $Y_n = 1-X$ when $n$ is even. Each sequence separately has the same Bernoulli$(1/2)$ law; hence $X_n \Rightarrow X$ and $Y_n \Rightarrow Y$. However,

* For odd $n$, $X_n + Y_n = 2X$ is supported on $\{0,2\}$,
* For even $n$, $X_n + Y_n = 1$ almost surely.

The distribution of $X_n + Y_n$ therefore oscillates between two incompatible measures, so it has no limit in distribution, and in particular it does not converge to $X+Y$.


4. Convergence in probability implies convergence in distribution

Goal. Show that $X_n \xrightarrow{\mathbb P} X$ implies $X_n \Rightarrow X$.

Let $f$ be bounded and continuous. We want to prove $\mathbb E[f(X_n)] \to \mathbb E[f(X)]$.

1. Tightness. Because $X_n \xrightarrow{\mathbb P} X$, the family $(X_n)$ is tight. Given $\varepsilon>0$, choose $M$ so that
$$
\sup_n \mathbb P(|X_n|>M) \le \varepsilon \,\text{ and }\, \mathbb P(|X|>M) \le \varepsilon.
$$

2. Uniform continuity. Since $f$ is continuous on the compact set $[-M,M]$, there exists $\delta>0$ with $|x-y|<\delta$ and $|x|,|y|\le M$ implying $|f(x)-f(y)|<\varepsilon$.

3. Main bound. Split according to the good event $G=\{|X_n-X|\le \delta, |X_n|\le M, |X|\le M\}$:
$$
|\mathbb E[f(X_n)]-\mathbb E[f(X)]| \le \varepsilon + 2\|f\|_\infty\, \mathbb P(G^c).
$$

4. Conclusion. Tightness gives $\mathbb P(|X_n|>M \text{ or } |X|>M) \le 2\varepsilon$, and convergence in probability ensures $\mathbb P(|X_n-X|>\delta) \to 0$. Hence $\mathbb P(G^c)\to 0$, forcing $|\mathbb E[f(X_n)]-\mathbb E[f(X)]| \to 0$.

Thus $X_n \Rightarrow X$, completing the implication.


