<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>連続確率空間と確率分布の性質</title>
    <style>
        body{font-family:"Helvetica Neue",Helvetica,Arial,sans-serif;line-height:1.7;color:#212121;margin:0;background:#fafafa;}
        .container{max-width:960px;margin:0 auto;padding:2rem 1rem 4rem;background:#fff;box-shadow:0 4px 12px rgba(0,0,0,.05);border-radius:12px;}
        h1,h2,h3{color:#1a237e;margin-top:1.2em;margin-bottom:.6em;}
        h1{font-size:1.9rem;text-align:center;}
        h2{font-size:1.45rem;border-left:6px solid #1a237e;padding-left:.55rem;}
        h3{font-size:1.2rem;}
        .formula{background:#f3f4ff;border-left:4px solid #5c6bc0;padding:.8rem 1.2rem;margin:1.2rem 0;border-radius:8px;overflow-x:auto;}
        .proof{background:#e8f5e9;border-left:4px solid #43a047;padding:.8rem 1.2rem;margin:1.2rem 0;border-radius:8px;overflow-x:auto;}
        details{margin-bottom:1rem;}
        details summary{cursor:pointer;font-weight:600;}
        ul{margin:0 0 1rem 1.2rem;}
        .navigation{display:flex;justify-content:center;gap:.8rem;margin:2.2rem 0;border-top:1px solid #e0e0e0;padding-top:1.5rem;flex-wrap:wrap;}
        .navigation a{padding:.65rem 1.5rem;background:#1a237e;color:#fff;text-decoration:none;border-radius:6px;transition:opacity .2s;}
        .navigation a:hover{opacity:.85;}
        footer{margin-top:3rem;color:#777;text-align:center;font-size:.9rem;}
        code{background:#eceff1;padding:2px 4px;border-radius:4px;}
    </style>
    <!-- MathJax configuration -->
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            tags: 'ams' // 数式番号のため
          },
          svg: {
            fontCache: 'global'
          }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <h1>連続確率空間と確率分布の性質</h1>
        <p>本節では連続確率分布の基本的な概念と代表例を取り上げ、それぞれの定義・性質・関連事項を整理します。以降 $X, Y$ は確率変数、$\mu$ は期待値、$\sigma^2$ は分散を表します。</p>

      <!-- ********** 連続確率空間の一般形 ********** -->
      <section id="continuous-space">
        <h2>連続確率空間の一般形</h2>
        <p>基本的な記法として、$P(\text{条件}) := P(\{\omega \in \Omega : \text{条件を満たす}\})$ のように書くことがあります。</p>
        <div class="formula">
            <p>
                指示関数を次で定義: $A\subset \mathbb{R}^d$について,
              $$\mathbf{1}_A(x) := \begin{cases}
                1 & (x \in A), \\
                0 & (x \notin A).
              \end{cases}$$
              さらに
              $$\int_A f(x)\,dx := \int_{\mathbb{R}^d} f(x)\,\mathbf{1}_A(x)\,dx.$$
            </p>
          </div>

        <p>$\Omega \subset \mathbb R^{d}$ を可測集合、確率密度関数 $p : \Omega \to \mathbb R$ を</p>
        <div class="formula">
          $$\int_{\Omega} p(x)\, \mathrm d x = 1$$
        </div>
        <p>を満たす関数とする。</p>
        <p>可測集合 $A \subset \Omega$ の確率は</p>
        <div class="formula">
          $$\mathbb P(A) = \int_{A} p(x_1,\ldots,x_d)  {\rm d}x_1\cdots {\rm d}x_d.$$
        </div>
        <p>
          よって $(\Omega, \mathcal B(\Omega), \mathbb P)$ を<strong>連続確率空間</strong>と呼ぶ。可測写像 $X : \Omega \to \mathbb R$ を確率変数とする時,
          $$\mathbb{E}[X] = \int_{\Omega} X(x_1,\ldots,x_d) p(x_1,\ldots,x_d) {\rm d}x_1\cdots {\rm d}x_d $$
        </p>
        <div class="formula">
            $\mathbb{P}(X\in I) = \displaystyle\int_{I} p_X(x)\,\mathrm d x$ と定義できる $p_X(x)$ が存在する時、$p_X(x)$ を $X$ の確率密度関数と呼ぶ。このとき、
            $$\mathbb{E}[X] = \int_{\mathbb{R}} x\,p_X(x)\,\mathrm d x $$
        </div>
        <div class="proof">
            <p>
            まず、$X(x_1,\dots,x_d)=\mathbf{1}_A(x_1,\dots,x_d)$ の場合には、
            $$
              \mathbb{E}[X]
              =\int_{\Omega} \mathbf{1}_A(x)\,p_X(x)\,\mathrm d x
              =\mathbb{P}(X\in A)
              =\int_A p_X(x)\,\mathrm d x,
            $$
            すなわち定義どおり成り立つ。
            </p>
            <p>
            次に、一般の非負可測関数 $X$ は単関数列
            $$
              X_n(x) \;=\;\sum_{k=1}^{N_n} a_k^{(n)}\,\mathbf{1}_{A_k^{(n)}}(x)
              \quad(a_k^{(n)}\ge0)
            $$
            で下から単調増加に近似できるので、単調収束定理より
            \begin{align*}
              \mathbb{E}[X]
              &=\lim_{n\to\infty}\mathbb{E}[X_n] \\
              &=\lim_{n\to\infty}\sum_{k=1}^{N_n}a_k^{(n)}\,
                \mathbb{P}(X\in A_k^{(n)}) \\
              &=\lim_{n\to\infty}\sum_{k=1}^{N_n}a_k^{(n)}
                \int_{A_k^{(n)}}p_X(x)\,\mathrm d x \\
              &=\int_{\mathbb{R}}x\,p_X(x)\,\mathrm d x.
            \end{align*}
            最後に、符号付き関数も正負分解 $X=X^+-X^-$ と単関数近似を組み合わせることで同様に示せる。
            </p>
        </div>
      </section>

      <section id="definition-of-distribution">
        <h2>分布とは？ 📊</h2>
        <p>
            確率空間 $(\Omega, \mathcal{F}, P)$ 上の確率変数 $X$ を考えます。
            任意の（ボレル）集合 $A \subset \mathbb{R}$ に対して、$X$ が $A$ に値を取る確率 $P(X \in A)$ を考えることができます。
            この対応 $A \mapsto P(X \in A)$ を $X$ の<strong>分布</strong> (または確率法則) と呼びます。
        </p>
        <p>
            2つの確率変数 $X$ と $Y$ (これらは異なる確率空間で定義されていても構いません) が、
            任意の（ボレル）集合 $A$ に対して $P(X \in A) = P(Y \in A)$ を満たすとき、$X$ と $Y$ は<strong>同じ分布を持つ</strong> (または同一分布に従う) といい、$X \overset{d}{=} Y$ と書きます。
        </p>
      </section>

      <section id="joint-pdf">
        <h2>同時確率密度関数</h2>
        <p>同一の確率空間で定義された2つの連続型確率変数 $X, Y$ を考えます。
        ある非負関数 $p_{X,Y}(x,y)$ が存在し、任意の（ボレル）集合 $C \subset \mathbb{R}^2$ に対して</p>
        <div class="formula">
            $$P((X,Y) \in C) = \iint_C p_{X,Y}(x,y) \,dx\,dy$$
        </div>
        <p>が成り立ち、かつ $\iint_{\mathbb{R}^2} p_{X,Y}(x,y) \,dx\,dy = 1$ であるとき、$p_{X,Y}(x,y)$ を $(X,Y)$ の<strong>同時確率密度関数</strong>と呼びます。</p>
      </section>

      <section id="definition-of-independence">
          <h2>独立の定義 🔗</h2>
          <p>2つの確率変数 $X, Y$ が<strong>独立</strong>であるとは、任意の（ボレル）集合 $A, B \subset \mathbb{R}$ に対して次が成り立つことをいいます。</p>
          <div class="formula">
              $$P(X \in A, Y \in B) = P(X \in A) P(Y \in B)$$
          </div>
      </section>

      <section id="proof-independence-factorization">
        <h2>確率密度関数の分解による独立性の証明</h2>
        <p>
            2つの連続型確率変数 $X, Y$ の同時確率密度関数を $p_{X,Y}(x,y)$ とします。
            もし、$p_{X,Y}(x,y)$ が2つの非負関数 $h_X(x)$ と $h_Y(y)$ の積として $p_{X,Y}(x,y) = h_X(x)h_Y(y)$ と書けるならば、$X$ と $Y$ は独立であることを示します。
            ここで、$h_X(x)$ や $h_Y(y)$ がそれぞれ積分して1になるとは限りません（つまり、これら自体が $X, Y$ の周辺確率密度関数であるとは限りません）。
            この証明は、ユーザーの指示「$p_{X,Y}(x,y) = p_X(x) p_Y(y)$とかけるときXとYが独立であることを示せ。その際$\int p_X=1$であることは仮定しない」に基づいています。ここで $p_X(x), p_Y(y)$ を $h_X(x), h_Y(y)$ と読み替えています。
        </p>
        <div class="proof">
            <p>
                $X$ の周辺確率密度関数 $p_X(x)$ と $Y$ の周辺確率密度関数 $p_Y(y)$ は、同時確率密度関数 $p_{X,Y}(x,y)$ から次のように定義されます。
                $$p_X(x) = \int_{-\infty}^{\infty} p_{X,Y}(x,y) \,dy$$
                $$p_Y(y) = \int_{-\infty}^{\infty} p_{X,Y}(x,y) \,dx$$
                仮定 $p_{X,Y}(x,y) = h_X(x)h_Y(y)$ を用いると、
                $$p_X(x) = \int_{-\infty}^{\infty} h_X(x)h_Y(y) \,dy = h_X(x) \int_{-\infty}^{\infty} h_Y(y) \,dy$$
                $$p_Y(y) = \int_{-\infty}^{\infty} h_X(x)h_Y(y) \,dx = h_Y(y) \int_{-\infty}^{\infty} h_X(x) \,dx$$
                ここで、$C_Y = \int_{-\infty}^{\infty} h_Y(y) \,dy$ および $C_X = \int_{-\infty}^{\infty} h_X(x) \,dx$ とおくと、
                $p_X(x) = C_Y h_X(x)$ および $p_Y(y) = C_X h_Y(y)$ となります。
                $p_{X,Y}(x,y)$ が確率密度関数であることから $h_X(x) \ge 0, h_Y(y) \ge 0$ であり、$C_X > 0, C_Y > 0$ となります（そうでなければ $p_X(x) \equiv 0$ または $p_Y(y) \equiv 0$ となり不適）。
            </p>
            <p>
                $p_{X,Y}(x,y)$ は同時確率密度関数なので、その全空間での積分は1です。
                $$\iint_{\mathbb{R}^2} p_{X,Y}(x,y) \,dx\,dy = \iint_{\mathbb{R}^2} h_X(x)h_Y(y) \,dx\,dy = 1$$
                この積分は次のように分離できます（フビニの定理より）。
                $$\left(\int_{-\infty}^{\infty} h_X(x) \,dx\right) \left(\int_{-\infty}^{\infty} h_Y(y) \,dy\right) = C_X C_Y = 1$$
            </p>
            <p>
                さて、$X$ と $Y$ が独立であることを示すために、$P(X \in A, Y \in B)$ を計算します。
                任意の（ボレル）集合 $A, B \subset \mathbb{R}$ に対して、
                \begin{align*}
                P(X \in A, Y \in B) &= \iint_{A \times B} p_{X,Y}(x,y) \,dx\,dy \\
                &= \iint_{A \times B} h_X(x)h_Y(y) \,dx\,dy \\
                &= \left(\int_A h_X(x) \,dx\right) \left(\int_B h_Y(y) \,dy\right)
                \end{align*}
                一方、$P(X \in A)$ と $P(Y \in B)$ は、
                $$P(X \in A) = \int_A p_X(x) \,dx = \int_A C_Y h_X(x) \,dx = C_Y \int_A h_X(x) \,dx$$
                $$P(Y \in B) = \int_B p_Y(y) \,dy = \int_B C_X h_Y(y) \,dy = C_X \int_B h_Y(y) \,dy$$
                したがって、
                $$P(X \in A)P(Y \in B) = \left(C_Y \int_A h_X(x) \,dx\right) \left(C_X \int_B h_Y(y) \,dy\right) = C_X C_Y \left(\int_A h_X(x) \,dx\right) \left(\int_B h_Y(y) \,dy\right)$$
                $C_X C_Y = 1$ であるから、
                $$P(X \in A)P(Y \in B) = \left(\int_A h_X(x) \,dx\right) \left(\int_B h_Y(y) \,dy\right)$$
                よって、$P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$ が成り立ち、$X$ と $Y$ は独立です。
            </p>
            <p>
                この証明では、$p_{X,Y}(x,y)$ が $h_X(x)h_Y(y)$ と分解できるという事実のみを用い、$h_X(x)$ や $h_Y(y)$ 自体が正規化されている（積分すると1になる）ことは仮定していません。
                それらが正規化されていなくても、$X$ と $Y$ の周辺確率密度関数 $p_X(x), p_Y(y)$ は、それぞれ $C_Y h_X(x), C_X h_Y(y)$ となり、正規化定数の積 $C_X C_Y = 1$ が成り立つため、独立性が示されます。
            </p>
        </div>
      </section>

      <section id="theorem-expectation-independent-rv">
        <h2>定理：独立な確率変数の関数の期待値 📜</h2>
        <p>確率変数 $X$ と $Y$ が独立 ($X \perp Y$) であるならば、任意の（可測な）関数 $f, g$ に対して次が成り立ちます。</p>
        <div class="formula">
            $$E[f(X)g(Y)] = E[f(X)]E[g(Y)]$$
        </div>
        <p>（ただし、各期待値が存在する場合）。</p>
        <div class="proof">
            <p><strong>証明の概略：</strong></p>
            <p>1. $f(x) = \mathbf{1}_A(x)$（指示関数）、$g(y) = \mathbf{1}_B(y)$ の場合：<br>
            $E[f(X)g(Y)] = E[\mathbf{1}_A(X)\mathbf{1}_B(Y)] = E[\mathbf{1}_{A \times B}(X,Y)] = P(X \in A, Y \in B)$.<br>
            $X \perp Y$ より $P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$.<br>
            また、$E[f(X)] = E[\mathbf{1}_A(X)] = P(X \in A)$、$E[g(Y)] = E[\mathbf{1}_B(Y)] = P(Y \in B)$.<br>
            よって、$E[f(X)g(Y)] = E[f(X)]E[g(Y)]$ が成り立つ。</p>
            <p>2. $f, g$ が単関数の場合：<br>
            $f(x) = \sum_i a_i \mathbf{1}_{A_i}(x)$, $g(y) = \sum_j b_j \mathbf{1}_{B_j}(y)$ と書ける。期待値の線形性と上記の結果より成り立つ。</p>
            <p>3. $f, g$ が非負の可測関数の場合：<br>
            それぞれを近似する非負単関数列 $\{f_n\}, \{g_n\}$ を考える。単調収束定理を用いて示すことができる。</p>
            <p>4. 一般の可測関数の場合：<br>
            関数を正部分と負部分に分け ($f = f^+ - f^-$、$g = g^+ - g^-$)、上記の結果を適用する。</p>
        </div>
    </section>

    <!-- ********** 一様分布 ********** -->
    <section id="uniform">
        <h2>一様分布</h2>
        <p>$\Omega = [0,1]$ とし、可測集合 $A \subset [0,1]$ に対して $P(A) = \int_{A} 1 \, d\omega$ と定める。</p>
        <p>確率変数 $X(\omega) = \omega$ とすると、区間 $[s,t] \subset [0,1]$ に対して</p>
        <div class="formula">
          $$P(s \le X \le t) = t - s.$$
        </div>
        <p>期待値と二乗期待値はそれぞれ</p>
        <div class="formula">
          $$\mathbb{E}[X] = \frac{1}{2}, \qquad \mathbb{E}[X^{2}] = \frac{1}{3}.$$
        </div>
        <h3>区間 $[a,b]$ 上の一様分布</h3>
        <p>$b > a$ に対し、$P(s \le X \le t) = \dfrac{t - s}{b - a}$ ($a \le s \le t \le b$) となる分布を一様分布 $\mathrm U(a,b)$ という。その確率密度関数は $f_X(x) = \frac{1}{b-a}$ ($a \le x \le b$) で、他では0。</p>
        <div class="formula">
          $$\mathbb{E}[X] = \frac{a + b}{2}, \qquad \mathrm{Var}(X) = \frac{(b - a)^{2}}{12}.$$
        </div>
      </section>
        <!-- -------------------------------------------------------------- -->
        <section id="normal">
            <h2>正規分布 (Normal Distribution)</h2>
            <p>$X\sim \mathcal{N}(\mu,\sigma^2)$ の確率密度関数（pdf）は</p>
            <div class="formula">
                $$f_{X}(x)= \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right), \qquad x\in \mathbb{R}.$$
            </div>
            <ul>
                <li>平均：$\mathbb{E}[X]=\mu$</li>
                <li>分散：$\mathrm{Var}(X)=\sigma^{2}$</li>
                <li>モーメント母関数：$M_{X}(t)=\exp\left(\mu t+\frac{1}{2}\sigma^{2}t^{2}\right)$</li>
                <li>標準化：$Z=\dfrac{X-\mu}{\sigma}\sim \mathcal{N}(0,1)$</li>
            </ul>
            <p>特に、平均 $\mu=0$, 分散 $\sigma^2=1$ の場合を<strong>標準正規分布</strong> $\mathcal{N}(0,1)$ といい、その確率密度関数は以下で与えられます。</p>
            <div class="formula">
                $$p(z) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}, \qquad z \in \mathbb{R}$$
            </div>
            <p>この積分 $\int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi}$ はガウス積分として知られています（これが正規化定数の導出に用いられます）。</p>
            <p>正規分布は多くの自然現象や社会現象における誤差の分布として現れます（中心極限定理により説明されることが多いです）。また、株価の変動モデル（例：幾何ブラウン運動）や量子力学など、多岐にわたる分野で基本的な分布として利用されます。</p>
            <details>
                <summary>平均と分散の導出（標準正規分布からのスケーリング）</summary>
                <div class="proof">
                    <p>$Z=\dfrac{X-\mu}{\sigma}$ は標準正規分布に従うので $\mathbb{E}[Z]=0$, $\mathrm{Var}(Z)=1$。よって
                    $$\mathbb{E}[X]=\mathbb{E}[\mu+\sigma Z]=\mu+\sigma\mathbb{E}[Z]=\mu,$$
                    $$\mathrm{Var}(X)=\mathrm{Var}(\mu+\sigma Z)=\sigma^{2}\mathrm{Var}(Z)=\sigma^{2}.$$
                    </p>
                </div>
            </details>
            <details>
                <summary>演習: 標準正規分布の期待値と分散の直接計算</summary>
                <div class="proof">
                    <p>標準正規分布 $Z \sim \mathcal{N}(0,1)$ の確率密度関数は $p(z) = \frac{1}{\sqrt{2\pi}} e^{-z^2/2}$ です。</p>
                    <p>1. 期待値 $\mathbb{E}[Z]$ を求めよ。</p>
                    <p>$\mathbb{E}[Z] = \int_{-\infty}^{\infty} z \frac{1}{\sqrt{2\pi}} e^{-z^2/2} dz$. 被積分関数 $z e^{-z^2/2}$ は奇関数（$f(-z)=-f(z)$）なので、対称区間 $(-\infty, \infty)$ での積分は0です。よって $\mathbb{E}[Z]=0$。</p>
                    <p>2. 二乗期待値 $\mathbb{E}[Z^2]$ を求めよ。</p>
                    <p>$\mathbb{E}[Z^2] = \int_{-\infty}^{\infty} z^2 \frac{1}{\sqrt{2\pi}} e^{-z^2/2} dz$. 部分積分を用います。 $u=z$, $dv = z e^{-z^2/2} dz$ とすると $v = -e^{-z^2/2}$。
                    \begin{align*} \mathbb{E}[Z^2] &= \frac{1}{\sqrt{2\pi}} \left[ -z e^{-z^2/2} \right]_{-\infty}^{\infty} - \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} (1)(-e^{-z^2/2}) dz \\ &= 0 + \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-z^2/2} dz \end{align*}
                    $\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-z^2/2} dz = 1$ (確率密度関数の全積分) なので、$\int_{-\infty}^{\infty} e^{-z^2/2} dz = \sqrt{2\pi}$。
                    よって $\mathbb{E}[Z^2] = \frac{1}{\sqrt{2\pi}} \cdot \sqrt{2\pi} = 1$。</p>
                    <p>したがって、$\mathrm{Var}(Z) = \mathbb{E}[Z^2] - (\mathbb{E}[Z])^2 = 1 - 0^2 = 1$。</p>
                </div>
            </details>
        </section>

        <!-- -------------------------------------------------------------- -->
        <section id="gamma">
            <h2>ガンマ分布 (Gamma Distribution)</h2>
            <p>形状パラメータ $k>0$、尺度パラメータ $\theta>0$ のとき $X\sim \mathrm{Gamma}(k,\theta)$ の pdf は</p>
            <div class="formula">
                $$f_{X}(x)=\frac{1}{\Gamma(k)\theta^{k}} x^{k-1} e^{-x/\theta}, \qquad x>0.$$
            </div>
            <p>この分布の定義にはガンマ関数 $\Gamma(k)$ が用いられます。ガンマ関数は以下で定義されます。</p>
            <div class="formula">
                $$\Gamma(k) = \int_0^\infty t^{k-1} e^{-t} dt, \qquad k > 0$$
            </div>
            <p>ガンマ関数は階乗の一般化であり、$\Gamma(k+1) = k\Gamma(k)$ や $\Gamma(n) = (n-1)!$ (nが正整数の場合) といった性質を持ちます。</p>
            <ul>
                <li>平均：$\mathbb{E}[X]=k\theta$</li>
                <li>分散：$\mathrm{Var}(X)=k\theta^{2}$</li>
                <li>加法性：独立な $X_{1}\sim \mathrm{Gamma}(k_{1},\theta)$, $X_{2}\sim \mathrm{Gamma}(k_{2},\theta)$ に対し $X_{1}+X_{2}\sim \mathrm{Gamma}(k_{1}+k_{2},\theta)$</li>
            </ul>
            <p><strong>関連する分布:</strong></p>
            <ul>
                <li>形状パラメータ $k=1$ のとき、ガンマ分布 $\mathrm{Gamma}(1, \theta)$ はレートパラメータ $\lambda = 1/\theta$ の指数分布 $\mathrm{Exp}(1/\theta)$ と一致します。</li>
                <li>もし $Z_1, \dots, Z_\nu$ が互いに独立な標準正規分布 $\mathcal{N}(0,1)$ に従う確率変数であるとき、その二乗和 $Q = \sum_{i=1}^\nu Z_i^2$ は自由度 $\nu$ のカイ二乗分布 $\chi^2(\nu)$ に従います。カイ二乗分布 $\chi^2(\nu)$ はガンマ分布 $\mathrm{Gamma}(\nu/2, 2)$ と同じ分布です。特に、$Z \sim \mathcal{N}(0,1)$ ならば、$Z^2 \sim \chi^2(1) \equiv \mathrm{Gamma}(1/2, 2)$ となります。</li>
            </ul>
            <details>
                <summary>平均の導出</summary>
                <div class="proof">
                    \begin{align*}
                    \mathbb{E}[X] &= \int_{0}^{\infty} x f_{X}(x)\,dx = \int_{0}^{\infty} x \frac{1}{\Gamma(k)\theta^{k}} x^{k-1} e^{-x/\theta}\,dx \\
                    &= \frac{1}{\Gamma(k)\theta^{k}}\int_{0}^{\infty} x^{k} e^{-x/\theta}\,dx
                    \end{align*}
                    ここで $y = x/\theta$ と置換すると、$x = y\theta, dx = \theta dy$。
                    \begin{align*}
                    \mathbb{E}[X] &= \frac{1}{\Gamma(k)\theta^{k}}\int_{0}^{\infty} (y\theta)^{k} e^{-y} \theta\,dy \\
                    &= \frac{\theta^{k+1}}{\Gamma(k)\theta^{k}}\int_{0}^{\infty} y^{k} e^{-y}\,dy \\
                    &= \frac{\theta}{\Gamma(k)} \Gamma(k+1) = \frac{\theta}{\Gamma(k)} k\Gamma(k) = k\theta.
                    \end{align*}
                </div>
            </details>
            <details>
                <summary>演習: ガンマ分布の二乗期待値と分散</summary>
                <div class="proof">
                    <p>$X \sim \mathrm{Gamma}(k, \theta)$ のとき、$\mathbb{E}[X]=k\theta$ でした。$\mathbb{E}[X^2]$ を求め、分散 $\mathrm{Var}(X)=k\theta^2$ を確認せよ。</p>
                    <p>$\mathbb{E}[X^2] = \int_0^\infty x^2 f_X(x) dx = \int_0^\infty x^2 \frac{1}{\Gamma(k)\theta^k} x^{k-1} e^{-x/\theta} dx$
                    \begin{align*} \mathbb{E}[X^2] &= \frac{1}{\Gamma(k)\theta^k} \int_0^\infty x^{k+1} e^{-x/\theta} dx \end{align*}
                    ここで、$y = x/\theta$ と置換すると $x=y\theta, dx=\theta dy$。
                    \begin{align*} \mathbb{E}[X^2] &= \frac{1}{\Gamma(k)\theta^k} \int_0^\infty (y\theta)^{k+1} e^{-y} \theta dy \\ &= \frac{\theta^{k+2}}{\Gamma(k)\theta^k} \int_0^\infty y^{k+1} e^{-y} dy \\ &= \frac{\theta^2}{\Gamma(k)} \Gamma(k+2) \end{align*}
                    $\Gamma(k+2) = (k+1)\Gamma(k+1) = (k+1)k\Gamma(k)$ なので、
                    $$\mathbb{E}[X^2] = \frac{\theta^2}{\Gamma(k)} (k+1)k\Gamma(k) = k(k+1)\theta^2$$
                    分散は、$\mathrm{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2 = k(k+1)\theta^2 - (k\theta)^2 = (k^2\theta^2 + k\theta^2) - k^2\theta^2 = k\theta^2$。</p>
                </div>
            </details>
        </section>

        <!-- -------------------------------------------------------------- -->
        <section id="exponential">
            <h2>指数分布 (Exponential Distribution)</h2>
            <p>レートパラメータ $\lambda>0$ のとき $X\sim \mathrm{Exp}(\lambda)$ の pdf は</p>
            <div class="formula">
                $$f_{X}(x)=\lambda e^{-\lambda x}, \qquad x>0.$$
            </div>
            <ul>
                <li>平均：$\mathbb{E}[X]=1/\lambda$</li>
                <li>分散：$\mathrm{Var}(X)=1/\lambda^{2}$</li>
                <li>無記憶性：$\mathbb{P}(X>s+t \mid X>t)=\mathbb{P}(X>s)$ for $s,t > 0$</li>
                <li>$\mathrm{Exp}(\lambda)$ は $\mathrm{Gamma}(1,1/\lambda)$ に一致 (形状パラメータ $k=1$, 尺度パラメータ $\theta=1/\lambda$)</li>
            </ul>
        </section>

        <section id="bivariate-normal">
            <h2>2次元正規分布 (Bivariate Normal Distribution) 🌌</h2>
            <p>多変量正規分布の最も簡単な例として2次元正規分布があります。
            特に、2つの確率変数 $X, Y$ の同時確率密度関数が以下で与えられる場合を考えます。</p>
            <div class="formula">
                $$p(x,y) = \frac{1}{2\pi} e^{-\frac{x^2+y^2}{2}}$$
            </div>
            <p>このとき、$X$ と $Y$ は独立であり、それぞれ標準正規分布 $\mathcal{N}(0,1)$ に従うことを示します。</p>
            <div class="proof">
                <p>$X$ の周辺確率密度関数 $p_X(x)$ は次のように計算できます。
                \begin{align*}
                p_X(x) &= \int_{-\infty}^{\infty} p(x,y) \,dy = \int_{-\infty}^{\infty} \frac{1}{2\pi} e^{-\frac{x^2+y^2}{2}} \,dy \\
                &= \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} \,dy
                \end{align*}
                ここで、積分 $\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} \,dy$ は標準正規分布の確率密度関数の全積分なので 1 です。
                したがって、$p_X(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$ となり、これは標準正規分布 $\mathcal{N}(0,1)$ の確率密度関数です。
                同様に、$Y$ の周辺確率密度関数 $p_Y(y) = \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}}$ となり、$Y \sim \mathcal{N}(0,1)$ です。
                </p>
                <p>
                与えられた同時確率密度関数は $p(x,y) = \left(\frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}\right) \left(\frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}}\right) = p_X(x)p_Y(y)$ と書けます。
                同時確率密度関数が周辺確率密度関数の積で書けるため、「確率密度関数の分解による独立性の証明」セクションの結果より、$X$ と $Y$ は独立です。
                </p>
            </div>
        </section>

        <section id="other-multivariate">
            <h2>他の多次元分布の例 🌐</h2>
            <p>同時確率密度関数が次のように与えられる分布を考えます。</p>
            <div class="formula">
                $$p(x,y) = \frac{1}{4} e^{-|x+y|-|y|}$$
            </div>
            <p><strong>課題例:</strong></p>
            <ul>
                <li>この分布に従う確率変数 $(X,Y)$ を考えます。変数変換 $U = X+Y, V = Y$ を行い、$U, V$ の同時確率密度関数 $p_{U,V}(u,v)$ を求めなさい。</li>
                <li>$U$ と $V$ が独立である ($U \perp V$) ことを示しなさい。</li>
                <li>上記の独立性を用いて $E[XY]$ を計算しなさい。ヒント: $X=U-V, Y=V$。</li>
            </ul>
        </section>

        <!-- -------------------------------------------------------------- -->
        <div class="navigation">
            <a href="class5.html">前のクラス</a>
            <a href="../index.html">ホーム</a>
            <a href="class7.html">次のクラス &rarr;</a>
        </div>

        <footer>&copy; 2025 確率論講義 - すべての権利を留保します</footer>
    </div>
</body>
</html>